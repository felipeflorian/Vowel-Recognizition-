{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FF_JM_WL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","<table>\n","    <tr>\n","        <td>&nbsp;</td>\n","        <td>\n","            <h1 style=\"color:blue;text-align:left\">Aprendizaje Automático de Maquina: Proyecto 2 </h1></td>\n","        <td>\n","            <table><tr>\n","            <tp><p style=\"font-size:150%;text-align:center\">Redes Neuronales</p></tp>\n","                <tp><p style=\"font-size:100%;text-align:center\">Andrés Florián </p></tp>\n","                <tp><p style=\"font-size:100%;text-align:center\">Joseph Mancera </p></tp>\n","                <tp><p style=\"font-size:100%;text-align:center\">William Lizarazo</p></tp>\n","            </tr></table>\n","        </td>\n","    </tr>\n","</table>\n","\n","---\n","\n","11 de Mayo de 2022"],"metadata":{"id":"_GUO25Lw1jQv"}},{"cell_type":"markdown","source":["# Proyecto II\n","\n","El objetivo es generar una red neuronal que pueda reconocer que vocal ha pronunciado una persona.\n","\n","Para la realización de este proyecto deben:\n","\n","1. Crear una base de datos con señales de personas diciendo las diferentes vocales. Tenga en cuenta las siguientes consideraciones:\n","    * Una red aprende con base en la información que le proporcionemos, asi que al crear la base de datos tenga en cuenta todas las posibilidades de como se pueden mencionar esas vocales. Además la base de datos debe contener diferentes muestras de diferentes personas, incluyendo niños, niñas, mujeres y hombres. El objetivo es generalizar, no memorizar patrones, para esto debemos darle suficiente información a la red.\n","    * Una señal de voz puede tener entre 8000 y 40000 mmuestras por segundo de grabación. Esto es bastante para ser utilizado como entrada en la red. recuerde que entre mayor dimensionalidad en los datos de entrada, mayor será la complejidad de la red necesaria. Para disminuir la complejidad pueden usar la densidad espectral de potencia (PSD) utilizando un número fijo de bins frecuenciales. La PSD representa la distribucion en frecuencia de la potencia de la señal.   \n","2. Definir las arquitecturas de red que debe probar.\n","3. Escoger la red que mejor se desempeñe.\n","4. Evaluar el rendimiento de la red.\n","\n","Tenga en cuenta que todo el procedimiento debe ser lo mas amigable para probar. Es decir se debe poder probar con cualquier señal, se ingresa la señal de voz y el programa debería indicar que vocal se pronunció.\n","\n","Deben contestar las siguientes preguntas:\n","\n","1. ¿Qué puede concluir del comportamiento de la red y los datos proporcionados?\n","2. ¿Qué criterio utilizó para seleccionar la arquitectura de la red?\n","3. ¿Tiene la red el comportamiento esperado?, si no es así, ¿A qué cree que se debe esto?\n","4. ¿Qué le mejoraria al modelo que usted diseño?, ¿Cómo implementaría esas mejoras?\n","\n","## Qué se debe entregar:\n","\n","1. La base de datos generada.\n","2. Un notebook de Jupyter donde presentan todo el pipeline para el entrenamiento de la red. Cada parte debe ser explicada.\n","3. El notebook debe incluir una función que permita ingresar una muestra de entrada y se proporcione la salida, sin mas pasos intermedios. Por lo tanto en esta función se debe tener en cuenta el acondicionamiento de los datos, antes de ser introducidos en la red neuronal par arealizar las predicciones.\n","4. El notebook debe incluir las respuestas a las preguntas planteadas."],"metadata":{"id":"GNFJLX3d29Um"}},{"cell_type":"markdown","source":["## Lectura base de datos y preprocesamiento\n","\n","Primero son importadas las librerías a utilizar."],"metadata":{"id":"PaIQsZFx2_le"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import os\n","import scipy\n","from scipy.signal import welch\n","import librosa, librosa.display\n","from collections import Counter"],"metadata":{"id":"FmMBXMnu1rir","executionInfo":{"status":"ok","timestamp":1652319866732,"user_tz":300,"elapsed":206,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Se realiza la lectura del dataset. Para ello se importa drive y se toma la ruta base en donde se encuentra el dataset almacenado."],"metadata":{"id":"4TWzFdlL3OnC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Proyecto2/dataset/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X24k26lI3QM2","executionInfo":{"status":"ok","timestamp":1652319888178,"user_tz":300,"elapsed":21272,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}},"outputId":"28e31624-2a15-4c08-d437-11858cd88bed"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Normalizar los datos permiten un mejor procesamiento y un mejor tratamiento de los mismos. Por ello es definida la siguiente función."],"metadata":{"id":"v8S7KDrW3493"}},{"cell_type":"code","source":["def norm_(audio):\n","  mean_ = np.mean(audio)\n","  max_ = np.max(audio)\n","\n","  return (audio - mean_)/max_ "],"metadata":{"id":"65yn1fy24CPp","executionInfo":{"status":"ok","timestamp":1652319888179,"user_tz":300,"elapsed":6,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["La gráfica de la señal de los audios permite identificar patrones entre las vocales e encontrar similitudes en las mismas. Con los cuales, es posible ilustrar el umbral de cada vocal en un rango posible de aceptación. "],"metadata":{"id":"z1PefBpP-TAc"}},{"cell_type":"code","source":["def dict_data(audios, labels):\n","  dict_audios = {'A': [], 'E': [], 'I': [], 'O': [], 'U': []}\n","\n","  for i in range(len(audios)):\n","    f, psd = welch(audios[i], nperseg=960)\n","    dict_audios[vowels[labels[i]]].append(psd)\n","\n","  return dict_audios"],"metadata":{"id":"xEk1P3TY-GNv","executionInfo":{"status":"ok","timestamp":1652319888179,"user_tz":300,"elapsed":5,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Ahora, se asigna como etiqueta $0$, $1$, $2$, $3$ y $4$, a las vocales *A*, *E*, *I*, *O*,y *U*, respectivamente.. Por otra parte, es necesario eliminar el silencio de cada audio con el fin de disminuir la duración del mismo y eliminar información no útil. Se realizó una reducción en los datos, al no ser considerados los audios sin silencio con más de un segundo de duración. Puesto que esto le pondría mayor complejidad a la red."],"metadata":{"id":"uqYnjbDM3cYx"}},{"cell_type":"code","source":["# Variables definition\n","\n","vowels = ['A', 'E', 'I', 'O', 'U']\n","all_audios = []\n","final_audios = []\n","label_all = []\n","labels = []\n","audios_duration = []\n","\n","# Final data\n","\n","for v in vowels:\n","  l = vowels.index(v)\n","\n","  # Dataset lecture\n","\n","  for route in os.listdir(path + v):\n","    data_ = librosa.load(path + v + '/' + route)\n","    all_audios.append(data_)\n","    label_all.append(l)\n","    \n","    # Silence removal\n","\n","    audio = librosa.effects.trim(data_[0], top_db=8)[0] \n","    t_ = librosa.get_duration(audio)\n","\n","    # Normalization audios that its duration is minor or equal than 1\n","\n","    if t_ <= 1:\n","      final_audio = norm_(audio)\n","      labels.append(l)\n","      final_audios.append(final_audio)\n","      audios_duration.append(t_)"],"metadata":{"id":"ik33kjxB3jm0","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1652319888711,"user_tz":300,"elapsed":537,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}},"outputId":"cf78e5b7-b6af-4667-f3ec-f1b8584574e9"},"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a74e491963e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# Dataset lecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mroute\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdata_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mall_audios\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Proyecto2/dataset/A'"]}]},{"cell_type":"code","source":["print('Total audios: ', len(all_audios))\n","print('Audios considerados: ', len(final_audios))"],"metadata":{"id":"fd9CgJcU8pD7","executionInfo":{"status":"aborted","timestamp":1652319888695,"user_tz":300,"elapsed":234,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Así, tenemos una cantidad de $279$ audios de los $289$ de la base de datos, siendo estos con los que trabajaremos. Primero, fueron graficados el umbral de cada vocal con respecto al rango determinado entre los percentiles $5$, $95$ y la mediana. Con la función *dict_data* definida previamente. En donde, se crea un diccionario con la señal de cada audio por medio del método *welch* de *scipy.signal*."],"metadata":{"id":"kx3LMNJm9Bdu"}},{"cell_type":"code","source":["dict_audios = dict_data(final_audios, labels)\n","\n","fig, axes = plt.subplots(3, 2, figsize=(20, 20))\n","fig.delaxes(ax=axes[2, 1])\n","\n","for i, v in enumerate(dict_audios.keys()):\n","    med = np.median(dict_audios[v], axis=0)\n","    per5 = np.percentile(dict_audios[v], 5, axis=0)\n","    per95 = np.percentile(dict_audios[v], 95, axis=0)\n","    if i % 2 == 0:\n","        axes[i//2,0].semilogy(med, c='deepskyblue')\n","        axes[i//2,0].semilogy(per5, 'c--', c='navy')\n","        axes[i//2,0].semilogy(per95, 'c--', c='navy')\n","        axes[i//2,0].set_title(v)\n","        axes[i//2,0].legend(['Mediana', 'Percentil 5', 'Percentil 95'])\n","    else:\n","        axes[i//2,1].semilogy(med, c='deepskyblue')\n","        axes[i//2,1].semilogy(per5, 'c--', c='navy')\n","        axes[i//2,1].semilogy(per95, 'c--', c='navy')\n","        axes[i//2,1].set_title(v)\n","        axes[i//2,1].legend(['Mediana', 'Percentil 5', 'Percentil 95'])"],"metadata":{"id":"RSAJTHev9XEf","executionInfo":{"status":"aborted","timestamp":1652319888696,"user_tz":300,"elapsed":234,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para un mejor tratamiento de los datos y evitar problemas con respecto a las dimensiones de los mismos, se decide tomar como referencia la media de la duración de los audios, con el fin de ya sea acortar o alargar los mismos hasta dicho valor. Para ello es necesario definir un sample rate, tomamos por defecto $22050$. El cual es el recomendado por $\\texttt{python}$ para el tratamiento de audios de voz (consultado documentación función *stft* de la librería *librosa*)."],"metadata":{"id":"0dKRMAGE_Lq9"}},{"cell_type":"code","source":["mean_ = np.mean(audios_duration); mean_"],"metadata":{"id":"1rC49xQmBaZT","executionInfo":{"status":"aborted","timestamp":1652319888700,"user_tz":300,"elapsed":29,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_audios = []\n","sample_rate = 22050\n","\n","# Changing the audios \n","\n","for audio in final_audios:\n","  max_ =  int(mean_ * sample_rate)\n","  if audio.shape[0] > max_:\n","    new_audios.append(audio[:max_])\n","  else:\n","    new_audios.append(librosa.effects.time_stretch(audio, rate = audio.shape[0]/max_))"],"metadata":{"id":"0OpPvqtACKS3","executionInfo":{"status":"aborted","timestamp":1652319888701,"user_tz":300,"elapsed":28,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Dimensión datos: ', np.array(new_audios).shape)"],"metadata":{"id":"jzM3v6vnDbyI","executionInfo":{"status":"aborted","timestamp":1652319888701,"user_tz":300,"elapsed":28,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, fueron realizados los espectogramas de cada uno de los audios, puesto que los audios son representados en ventanas de tiempo, donde cada ventana o intervalo de tiempo es tratado con la transformada de Fourier permitiendo así una mejor representación de la señal de audio tanto visual como computacional. Lo cual implica un mejor procesamiento de los datos en la red neuronal. Dichos datos son en $3$ dimensiones los cuales dan características de la frecuencia e intensidad de la señal."],"metadata":{"id":"bZKtwiJ3W-LE"}},{"cell_type":"code","source":["spectograms = []\n","fourier_t = []\n","\n","for audio in new_audios:\n","  fourier_ = librosa.stft(audio, n_fft=512)\n","  mag, _ = librosa.magphase(fourier_)\n","  mel_ = librosa.feature.melspectrogram(S=mag, sr=sample_rate, n_fft=512, hop_length=23)\n","  spect = librosa.amplitude_to_db(mel_, ref=np.min)\n","  spectograms.append(spect)\n","  fourier_t.append(fourier_)"],"metadata":{"id":"Ku7gkdKxE4yP","executionInfo":{"status":"aborted","timestamp":1652319888702,"user_tz":300,"elapsed":29,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Dimensión final información: ', np.array(spectograms).shape)"],"metadata":{"id":"xXHKXw2JJUpJ","executionInfo":{"status":"aborted","timestamp":1652319888703,"user_tz":300,"elapsed":29,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definición dataset con *pandas*. Ingresando los datos recolectados previamente con los espectogramas."],"metadata":{"id":"3F0V2KSGMT9S"}},{"cell_type":"code","source":["data_ = []\n","\n","for i in range(len(spectograms)):\n","  data_.append([spectograms[i], labels[i]])\n","\n","df = pd.DataFrame(data_, columns=['data', 'label'])\n","df.head()"],"metadata":{"id":"bU7nQyKtMUcN","executionInfo":{"status":"aborted","timestamp":1652319888704,"user_tz":300,"elapsed":30,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline \n","\n","Se realiza la partición del dataset en entrenamiento, pruebas y validación. En donde, se considero un $75\\%$ de entrenamiento y un $25\\%$ de pruebas. "],"metadata":{"id":"fl_iS2doZH6t"}},{"cell_type":"code","source":["X = np.array(df['data'].tolist())\n","Y = np.array(df['label'].tolist())\n","\n","# Training and test separation\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=10)  \n","\n","# Training and validation separation\n","X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25, random_state=50)"],"metadata":{"id":"IG6bOhS-MrlA","executionInfo":{"status":"aborted","timestamp":1652319888704,"user_tz":300,"elapsed":30,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, a, b = X_train.shape"],"metadata":{"id":"1Nw22wVBNIaZ","executionInfo":{"status":"aborted","timestamp":1652319888705,"user_tz":300,"elapsed":31,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora, como se tienen datos en $3$ dimensiones y la red admite solo de $2$ son transformados en $2$ dimensiones. De igual manera, se definen como variables categóricas a las variables que contienen los *labels*.\n"],"metadata":{"id":"_pSTtS-bNJ6C"}},{"cell_type":"code","source":["# Reshape X data\n","X_train = X_train.reshape((len(X_train), a*b))\n","X_validation = X_validation.reshape((len(X_validation), a*b))\n","X_test = X_test.reshape((len(X_test), a*b))\n","\n","# Categorical Y data\n","Y_train = to_categorical(Y_train)\n","Y_validation = to_categorical(Y_validation)\n","Y_test = to_categorical(Y_test)"],"metadata":{"id":"6xLKZszuNMQy","executionInfo":{"status":"aborted","timestamp":1652319888706,"user_tz":300,"elapsed":31,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para el modelo, se tomo una red neuronal de una capa oculta con función de activación *sigmoid*. Una capa de salida de $5$ neuronas, ya que se busca clasificar un audio entre una de las cinco vocales. Adicionalmente, se toma como optimizador el algoritmo del gradiente descendiente con taza de aprendizaje del $0.0005$."],"metadata":{"id":"m5FJ0E8JNVN8"}},{"cell_type":"code","source":["model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Dense(300, activation='sigmoid', input_dim=X_train.shape[1] ))\n","model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","\n","opt = SGD(learning_rate=0.0005)\n","model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])"],"metadata":{"id":"8q_igQzXNXUc","executionInfo":{"status":"aborted","timestamp":1652319888706,"user_tz":300,"elapsed":31,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se entrena el modelo usando $500$ epocas y los vectores de validación definidos previamente."],"metadata":{"id":"mBOR7gU-csfW"}},{"cell_type":"code","source":["history = model.fit(X_train, Y_train, validation_data=(X_validation, Y_validation), epochs=500)"],"metadata":{"id":"yNpSCn5lNguy","executionInfo":{"status":"aborted","timestamp":1652319888707,"user_tz":300,"elapsed":32,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comportamiento de la red\n","\n","Primero, la matriz de confusión de los datos de prueba. Se encuentra que clasifica correctamente todas las A's y O's. Sin embargo, confunde ciertas E's con I's o viceversa. Por otra parte, es posible que se confundan las U's con O's. Esto se debe a la similitud en las señales de las vocales, lo podemos observar en las gráficas vistas previamente del rango del umbral. Donde es posible observar similitudes entre las vocales E e I y también O y U, debido al comportamiento de las mismas. "],"metadata":{"id":"SqgsjYkfeEG7"}},{"cell_type":"code","source":["y_pred = model.predict(X_test)\n","cm = confusion_matrix(np.argmax(Y_test, axis=1).tolist(),np.argmax(y_pred, axis=1).tolist())\n","sns.heatmap(cm, cmap='Blues', annot=True, square = True, linewidths = 1, linecolor = 'black',\n","             yticklabels=[\"A\",\"E\",\"I\",\"O\",\"U\"], xticklabels=[\"A\",\"E\",\"I\",\"O\",\"U\"])\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")"],"metadata":{"id":"9LFnCfhPOOnb","executionInfo":{"status":"aborted","timestamp":1652319888707,"user_tz":300,"elapsed":31,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En la gráfica, de precisión, se observa como el entrenamiento se empieza a ajustar a la red de mejor manera que la validación. Asimismo, se aproximan rapidamente hacia el $0.8$ alrededor de la epoca $50$, i.e., puede que no sean necesarias la cantidad de epocas propuestas, la red podría trabajar de igual manera con aproximadamente $250$ epocas, ya que en estos valores tiende a acercarse la validación al $0.95$.\n","\n","Por otro lado, no se presenta *overfitting* ni *underfitting*, esto se identifica en la gráfica, ya que no se visualizan saltos en la precisión de la validación y el entrenamiento, en cambio, estos mantienen un ascenso gradual, descartando así *underfitting*. Igualmente, la precisión de los datos de entrenamiento aumenta junto con la precisión de los datos de validación, significando así la no presencia de *overfitting*."],"metadata":{"id":"0p47pq7ye_i5"}},{"cell_type":"code","source":["epochs = range(1, 501)\n","plt.plot(epochs, history.history['accuracy'], c='deepskyblue', label='Training accuracy')\n","plt.plot(epochs, history.history['val_accuracy'], c='Navy', label='Validation accuracy')\n","plt.legend()"],"metadata":{"id":"Wm89WOH3ORVJ","executionInfo":{"status":"aborted","timestamp":1652319888708,"user_tz":300,"elapsed":32,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora, en la gráfica de perdida se evidencia una pequeña distancia entre las curvas de entrenamiento y validación, siendo esto positivo para la red. También, ambas curvas decrecen hasta $0.4$ paulatinamente."],"metadata":{"id":"DXmoTPvif5Kc"}},{"cell_type":"code","source":["epochs = range(1, 501)\n","plt.plot(epochs, history.history['loss'], c='deepskyblue', label='Training Loss')\n","plt.plot(epochs, history.history['val_loss'], c='Navy', label='Validation Loss')\n","plt.legend()"],"metadata":{"id":"qBk61-S5Oosb","executionInfo":{"status":"aborted","timestamp":1652319888708,"user_tz":300,"elapsed":32,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pruebas\n","\n","Es definida la siguiente función que recibe como parametro la ruta en donde se encuentra el audio y realiza el proceso hecho anteriormente para este con el fin de que pueda ser aceptado en la red para su predicción."],"metadata":{"id":"1BDtGoPPiOhZ"}},{"cell_type":"code","source":["def predict_value(path):\n","  data_ = librosa.load(path)\n","  audio = librosa.effects.trim(data_[0], top_db=8)[0] \n","  t_ = librosa.get_duration(audio)\n","  audio = norm_(audio)\n","  max_ =  int(mean_ * sample_rate)\n","  if audio.shape[0] > max_:\n","      audio = audio[:max_]\n","  else:\n","      audio = librosa.effects.time_stretch(audio, rate = audio.shape[0]/max_)\n","\n","  fourier_ = librosa.stft(audio, n_fft=512)\n","  mag, _ = librosa.magphase(fourier_)\n","  mel_ = librosa.feature.melspectrogram(S=mag, sr=sample_rate, n_fft=512, hop_length=23)\n","  spect = librosa.amplitude_to_db(mel_, ref=np.min)\n","  input = np.reshape(spect, (1,spect.shape[0]*spect.shape[1]))\n","\n","  pred = model.predict(input)\n","  value = np.argmax(pred,axis=1)\n","\n","  return vowels[value[0]]"],"metadata":{"id":"YSfYj4A4PIjE","executionInfo":{"status":"aborted","timestamp":1652319888710,"user_tz":300,"elapsed":34,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para las pruebas se toma un dataset distinto al usado en el entrenamiento de la red, en donde se realiza una predicción para cada uno de los audios almacenados. Dicha información es guardada en un diccionario."],"metadata":{"id":"Oyd1_n8ZigCu"}},{"cell_type":"code","source":["dict_predictions = {'A': [], 'E': [], 'I': [], 'O': [], 'U': []}\n","path = '/content/drive/MyDrive/Proyecto2/Letras/'\n","\n","for v in dict_predictions.keys():\n","  for route in os.listdir(path + v):\n","    predict = predict_value(path + v + '/' + route)\n","    dict_predictions[v].append(predict)"],"metadata":{"id":"91DqPxflrQt_","executionInfo":{"status":"aborted","timestamp":1652319888710,"user_tz":300,"elapsed":32,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, se muestra el desempeño de la red con respecto a cada vocal. El cual es bastante bueno! :)"],"metadata":{"id":"92yhGAl1i63N"}},{"cell_type":"code","source":["print('------------')\n","for v in dict_predictions.keys():\n","  print()\n","  total = len(dict_predictions[v])\n","  clasified =  Counter(dict_predictions[v])\n","  print('Se clasificaron correctamente {0} {1}s de {2}.'.format(clasified[v], v, total))\n","  print()\n","  print('En general, fueron clasificadas de la siguiente manera: {}'.format(clasified))\n","  print()\n","  print('------------')"],"metadata":{"id":"6lnOGReyPkJw","executionInfo":{"status":"aborted","timestamp":1652319888711,"user_tz":300,"elapsed":31,"user":{"displayName":"Joseph Mancera","userId":"07218951686344995709"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección teórica\n","\n","### 1. ¿Qué puede concluir del comportamiento de la red y los datos proporcionados?\n","\n","La red tiene un gran desempeño, esto se evidencia en las pruebas realizadas tanto en la matriz de confusión como en el *test* con cada uno de los audios tomados de la base de datos de pruebas. En donde, el peor rendimiento se evidencia en la vocal *I*, pues fueron clasificados correctamente $42$ audios de $66$, siendo confundidos, en general, con la vocal *E*.\n","\n","Dichos resultados se deben al procesamiento de los datos realizados antes del entrenamiento. Los cuales permiten encapsular y generalizar de gran manera la información tomada para el entrenamiento. También, las similitudes con las señales de cada vocal presentadas por medio de las gráficas, se constata en las pruebas, pues las vocales *E* e *I*, *U* y *O* son confundidas en la red.\n","\n","### 2. ¿Qué criterio utilizó para seleccionar la arquitectura de la red?\n","\n","Se utilizó una red neuronal con una capa oculta. Se determinó utilizar una capa oculta solamente, ya que hay realizar distintas pruebas para obtener el mejor comportamiento de la red se vió que al aumentar el número de capas, puede que la precisión del modelo se afecte. Sin embargo, es posible adaptar el modelo a las mismas.\n","\n","Para agregar, se tomo como optimizador, el gradiente descendiente, y en la última capa se toman 5 neuronas, ya que se buscan clasificar las 5 vocales, con la activación *softmax* (activación multi-clase). Finalmente, el número de neuronas se obtuvo al ir aumentando estas hasta el punto en el que no se presentará *overfitting* ni *underfitting*.\n","\n","### 3. ¿Tiene la red el comportamiento esperado?, si no es así, ¿A qué cree que se debe esto?\n","\n","La red tiene el comportamiento esperado, es capaz de diferenciar en la gran mayoria de los casos cada una de las vocales, en el *testing* se obtiene una clasificación optima en más del $60\\%$ de los casos. Por otra parte, se esperaba que confundiera las vocales *E* e *I* y *O* y *U*, debido a la similitud en sus señales. No obstante, no fue previsto que la vocal *A* fuera clasificada como cualquiera de las otras vocales (veáse sección de pruebas). Empero, la confusión con la vocal *O* puede ser causada a que ambas con vocales abiertas. \n","\n","### 4. ¿Qué le mejoraria al modelo que usted diseño?, ¿Cómo implementaría esas mejoras?\n","\n","Al modelo se le implementarían las siguientes mejoras: \n","\n","*   Explorar más funciones de activación para mejorar el accuracy o explorar nuevos comportamientos en la red.\n","*   Tomar mayor cantidad de muestras, en un ambiente estandarizado. Para tener una mayor uniformidad en los datos.\n","*   Implementar regularizaciones a la red.\n","*   Determinar un mejor procesamiento de los datos el cual ayude a evitar confusiones entre las vocales.\n","*   Implementar una interfaz interactiva con el usuario.\n","\n","\n"],"metadata":{"id":"n4mZg1-TjJRZ"}},{"cell_type":"markdown","source":["## Datasets\n","\n","1.   Bases de datos proporcionadas por estudiantes de pregrado, que cursaron la asignatura en 2021-2, o que lo estan cursando en 2022-1.\n","\n"],"metadata":{"id":"g9G7tlxKQrwy"}}]}